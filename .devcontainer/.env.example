# =============================================================================
# Configuration Claude DevContainer - LLM Entreprise
# =============================================================================
# Copiez ce fichier en .env et configurez les valeurs
# cp .env.example .env
# =============================================================================

# -----------------------------------------------------------------------------
# PHASE TEST 1 : Anthropic (validation du proxy)
# -----------------------------------------------------------------------------
# Décommentez cette section pour tester avec les modèles Claude natifs
# -----------------------------------------------------------------------------
# PREFERRED_PROVIDER="anthropic"
# ANTHROPIC_API_KEY="sk-ant-votre-clé-anthropic"
# BIG_MODEL="claude-sonnet-4-20250514"
# SMALL_MODEL="claude-haiku-4-20250514"

# -----------------------------------------------------------------------------
# PHASE TEST 2 : Mammouth.ai (LLM entreprise)
# -----------------------------------------------------------------------------
# Configuration pour utiliser les modèles Mistral/Codestral via Mammouth.ai
# -----------------------------------------------------------------------------
PREFERRED_PROVIDER="openai"
OPENAI_BASE_URL="https://api.mammouth.ai/v1"
OPENAI_API_KEY="votre-clé-mammouth-ai"

# Mapping des modèles (Phase 1 - 2 modèles)
BIG_MODEL="codestral-2508"
SMALL_MODEL="mistral-small-3.2-24b-instruct"

# -----------------------------------------------------------------------------
# PHASE 2 (future) : Support 3 modèles
# -----------------------------------------------------------------------------
# Nécessite modification de server.py
# -----------------------------------------------------------------------------
# OPUS_MODEL="codestral-2508"
# SONNET_MODEL="mistral-medium-2312"
# HAIKU_MODEL="mistral-small-3.2-24b-instruct"
